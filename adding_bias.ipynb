{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "\n",
    "    def __init__(self, num_inputs=3, hidden_layers =[2], num_outputs=2):\n",
    "\n",
    "        # Initiate array of layers size\n",
    "        self.num_inputs = num_inputs\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        # create a generic representation of the layers\n",
    "        layers = [num_inputs] + hidden_layers + [num_outputs]\n",
    "\n",
    "        # create random connection weights for the layers\n",
    "        self.weights = [np.random.rand(layers[i] + 1, layers[i + 1]) for i in range(len(layers) - 1)]\n",
    "\n",
    "        # save derivatives per layer\n",
    "        self.derivatives = [np.zeros((layers[i] + 1, layers[i + 1])) for i in range(len(layers) - 1)]\n",
    "\n",
    "        # save activations per layer\n",
    "\n",
    "        #print(layers)\n",
    "        #self.activations = [np.zeros(layers[i] + 1) if i!= len(layers)-1 else np.zeros(layers[i]) for i in range(len(layers))]\n",
    "        self.activations = [np.zeros(layers[i] + 1) for i in range(len(layers))]\n",
    "\n",
    "    def forward_propagate(self, inputs):\n",
    "\n",
    "        # Store the input layer activations\n",
    "        activations = inputs\n",
    "        self.activations[0][:-1] = activations\n",
    "        self.activations[0][-1] = 1\n",
    "        activations = np.append(activations, 1)\n",
    "\n",
    "        #print(self.activations)\n",
    "\n",
    "        for i, w in enumerate(self.weights):\n",
    "            # Calculate inputs h\n",
    "            #print(activations, i)\n",
    "            net_inputs = np.dot(activations, w)\n",
    "\n",
    "            # Calculate the activations\n",
    "            activations = self._sigmoid(net_inputs)\n",
    "\n",
    "            # Save the activation for the current layer (if weights at W1, store a2)\n",
    "            self.activations[i+1][:-1] = activations\n",
    "            self.activations[i+1][-1] = 1\n",
    "            activations = np.append(activations, 1)\n",
    "\n",
    "            \n",
    "            # if i != len(self.weights) - 1:\n",
    "            #     self.activations[i+1][:-1] = activations\n",
    "            #     self.activations[i+1][-1] = 1\n",
    "            #     activations = np.append(activations, 1)\n",
    "            # else:\n",
    "            #     self.activations[i+1] = activations\n",
    "            \n",
    "        return self.activations[-1][:-1]\n",
    "\n",
    "\n",
    "    def gradient_descent(self, learning_rate, debug = False):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * self.derivatives[i]\n",
    "\n",
    "\n",
    "    def train(self, inputs, targets, epochs = 10, learning_rate = 0.1, batch_size = 0):\n",
    "\n",
    "        for i in range(epochs):\n",
    "            sum_error = 0\n",
    "            for j in range(len(inputs)):\n",
    "\n",
    "                # Forward propagate\n",
    "                output = self.forward_propagate(inputs[j])\n",
    "\n",
    "                # Find derivative of loss function with respect to output layer\n",
    "                loss_derivative = output - targets[j]\n",
    "\n",
    "                # Back propagate to find the derivatives\n",
    "                self.back_propagate(loss_derivative)\n",
    "\n",
    "                # Gradient descent to update weights\n",
    "                self.gradient_descent(learning_rate)\n",
    "\n",
    "                # Calculate the error at this iteration\n",
    "                sum_error += self._mse(targets[j], output)\n",
    "            \n",
    "            # report error\n",
    "            print(f\"Epoch {i}, Mean Squared Error: {sum_error/len(inputs)}\")\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _mse(target, output):\n",
    "        return np.average((target - output) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid_derivative(x):\n",
    "        return x * (1.0 - x)\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(x):\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "\n",
    "    def back_propagate(self, error, debug = False):\n",
    "\n",
    "        for i in reversed(range(len(self.derivatives))):\n",
    "            # If w2, take derivative of a3\n",
    "            # define delta as: dE/da_(i+1) * s'(a_i+1)\n",
    "            #print(\"At index:\", i)\n",
    "            #print(\"Error\", error)\n",
    "\n",
    "            #if i == len(self.derivatives) - 1:\n",
    "            delta = error * self._sigmoid_derivative(self.activations[i+1][:-1])\n",
    "            #else:\n",
    "                #delta = error * self._sigmoid_derivative(self.activations[i+1])\n",
    "\n",
    "            # Converts delta from array [0.1, 0.2] to 1x2 matrix [[0.1, 0.2]]\n",
    "            delta_reshaped = delta.reshape(delta.shape[0],-1).T\n",
    "\n",
    "            # get activations for current layer\n",
    "            #current_activations = self.activations[i]\n",
    "\n",
    "            #print(\"delta r: \", delta_reshaped)\n",
    "\n",
    "            # Converts a_i from array [0.1, 0.2] to 2x1 matrix [[0.1], [0.2]]\n",
    "            #if i == len(self.derivatives) -1:\n",
    "            current_activations = self.activations[i].reshape(self.activations[i].shape[0], -1)\n",
    "            #else:\n",
    "                #current_activations = self.activations[i][:-1].reshape(self.activations[i][:-1].shape[0], -1)\n",
    "\n",
    "            #print(\"currenct: \", current_activations)\n",
    "\n",
    "            # Get the matrix of derivatives for W_i\n",
    "            self.derivatives[i] = np.dot(current_activations, delta_reshaped)\n",
    "\n",
    "            # error for prev layer as: dE/da_(i+1) * s'(a_i+1) * W_i or delta * W_i\n",
    "            error = np.dot(delta, self.weights[i][:-1].T)\n",
    "\n",
    "            if debug:\n",
    "                print(f\"Derivatives for W{i}: {self.derivatives[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04666414, 0.21413907])"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define data\n",
    "items = np.array([[random()/2 for _ in range(2)] for _ in range(1000)])\n",
    "targets = np.array([[i[0] + i[1]] for i in items])\n",
    "\n",
    "items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2608032])"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0.]), array([0., 0., 0.]), array([0., 0.])]"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLP(num_inputs = 2, hidden_layers = [2], num_outputs = 1)\n",
    "mlp.activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77562787])"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.forward_propagate(np.array([3, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.]])]"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Mean Squared Error: 0.04483743491999746\n",
      "Epoch 1, Mean Squared Error: 0.041473861304714296\n",
      "Epoch 2, Mean Squared Error: 0.041006684268530644\n",
      "Epoch 3, Mean Squared Error: 0.0404720658201423\n",
      "Epoch 4, Mean Squared Error: 0.0398426606453356\n",
      "Epoch 5, Mean Squared Error: 0.039085531115876294\n",
      "Epoch 6, Mean Squared Error: 0.03816068736705501\n",
      "Epoch 7, Mean Squared Error: 0.03702040364806856\n",
      "Epoch 8, Mean Squared Error: 0.03561065668091661\n",
      "Epoch 9, Mean Squared Error: 0.0338768140821938\n",
      "Epoch 10, Mean Squared Error: 0.03177567119538672\n",
      "Epoch 11, Mean Squared Error: 0.029293480005309132\n",
      "Epoch 12, Mean Squared Error: 0.026464291176186393\n",
      "Epoch 13, Mean Squared Error: 0.023378546490676378\n",
      "Epoch 14, Mean Squared Error: 0.020174322883443063\n",
      "Epoch 15, Mean Squared Error: 0.017012434859381483\n",
      "Epoch 16, Mean Squared Error: 0.014044480751582173\n",
      "Epoch 17, Mean Squared Error: 0.01138542539164104\n",
      "Epoch 18, Mean Squared Error: 0.009099514913805616\n",
      "Epoch 19, Mean Squared Error: 0.007201623635081255\n",
      "Epoch 20, Mean Squared Error: 0.005669364153324416\n",
      "Epoch 21, Mean Squared Error: 0.004458645131790107\n",
      "Epoch 22, Mean Squared Error: 0.003517108349715544\n",
      "Epoch 23, Mean Squared Error: 0.0027931750136019934\n",
      "Epoch 24, Mean Squared Error: 0.0022408800665828853\n",
      "Epoch 25, Mean Squared Error: 0.0018216878800585668\n",
      "Epoch 26, Mean Squared Error: 0.0015045341100248509\n",
      "Epoch 27, Mean Squared Error: 0.0012650131408951726\n",
      "Epoch 28, Mean Squared Error: 0.0010842726814624692\n",
      "Epoch 29, Mean Squared Error: 0.0009479100877625671\n",
      "Epoch 30, Mean Squared Error: 0.0008449994079665054\n",
      "Epoch 31, Mean Squared Error: 0.0007672878944262002\n",
      "Epoch 32, Mean Squared Error: 0.0007085573365160007\n",
      "Epoch 33, Mean Squared Error: 0.000664128189590765\n",
      "Epoch 34, Mean Squared Error: 0.0006304801950614384\n",
      "Epoch 35, Mean Squared Error: 0.0006049647756114844\n",
      "Epoch 36, Mean Squared Error: 0.0005855882143093489\n",
      "Epoch 37, Mean Squared Error: 0.0005708487143488677\n",
      "Epoch 38, Mean Squared Error: 0.0005596141429037294\n",
      "Epoch 39, Mean Squared Error: 0.0005510303483886788\n",
      "Epoch 40, Mean Squared Error: 0.0005444523936387493\n",
      "Epoch 41, Mean Squared Error: 0.0005393929458513542\n",
      "Epoch 42, Mean Squared Error: 0.0005354835091751912\n",
      "Epoch 43, Mean Squared Error: 0.0005324452748530725\n",
      "Epoch 44, Mean Squared Error: 0.0005300671796817213\n",
      "Epoch 45, Mean Squared Error: 0.0005281893727608335\n",
      "Epoch 46, Mean Squared Error: 0.0005266907447248755\n",
      "Epoch 47, Mean Squared Error: 0.0005254795122053711\n",
      "Epoch 48, Mean Squared Error: 0.0005244861027284836\n",
      "Epoch 49, Mean Squared Error: 0.0005236577736867523\n"
     ]
    }
   ],
   "source": [
    "mlp.train(items, targets, epochs=50, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.43067377])"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.forward_propagate(np.array([0.123, 0.321]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "metadata": {
   "interpreter": {
    "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
   }
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "44e4ef2e98939612f32f24531eaa0adce7e0bc2c75fe1e0ac5cf87184e810651"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
