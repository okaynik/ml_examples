{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import pickle\n",
    "np.set_printoptions(linewidth = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "\n",
    "    def __init__(self, num_inputs=3, hidden_layers =[2], num_outputs=2):\n",
    "\n",
    "        # Initiate array of layers size\n",
    "        self.num_inputs = num_inputs\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        # create a generic representation of the layers\n",
    "        layers = [num_inputs] + hidden_layers + [num_outputs]\n",
    "\n",
    "        # create random connection weights for the layers\n",
    "        self.weights = [(np.random.uniform(low = -0.5, high = 0.5, size = (layers[i] + 1, layers[i + 1]))) for i in range(len(layers) - 1)]\n",
    "\n",
    "        # save derivatives per layer\n",
    "        self.derivatives = [np.zeros((layers[i] + 1, layers[i + 1])) for i in range(len(layers) - 1)]\n",
    "\n",
    "        # save activations per layer\n",
    "\n",
    "        #print(layers)\n",
    "        #self.activations = [np.zeros(layers[i] + 1) if i!= len(layers)-1 else np.zeros(layers[i]) for i in range(len(layers))]\n",
    "        self.activations = [np.zeros(layers[i] + 1) for i in range(len(layers))]\n",
    "\n",
    "    \n",
    "    def back_propagate(self, error, debug = False):\n",
    "\n",
    "        for i in reversed(range(len(self.derivatives))):\n",
    "            # If w2, take derivative of a3\n",
    "            # define delta as: dE/da_(i+1) * s'(a_i+1)\n",
    "            if i == len(self.derivatives) - 1:\n",
    "                #print('here')\n",
    "                delta = error\n",
    "            else:\n",
    "                delta = error * self._sigmoid_derivative(self.activations[i+1][:-1])\n",
    "\n",
    "            # Converts delta from array [0.1, 0.2] to 1x2 matrix [[0.1, 0.2]]\n",
    "            delta_reshaped = delta.reshape(delta.shape[0],-1).T\n",
    "\n",
    "            # get activations for current layer\n",
    "            #current_activations = self.activations[i]\n",
    "\n",
    "            # Converts a_i from array [0.1, 0.2] to 2x1 matrix [[0.1], [0.2]]\n",
    "            current_activations = self.activations[i].reshape(self.activations[i].shape[0], -1)\n",
    "\n",
    "            # Get the matrix of derivatives for W_i\n",
    "            self.derivatives[i] = np.dot(current_activations, delta_reshaped)\n",
    "\n",
    "            # error for prev layer as: dE/da_(i+1) * s'(a_i+1) * W_i or delta * W_i\n",
    "            error = np.dot(delta, self.weights[i][:-1].T)\n",
    "\n",
    "            if debug:\n",
    "                print(f\"Derivatives for W{i}: {self.derivatives[i]}\")\n",
    "\n",
    "    def forward_propagate(self, inputs):\n",
    "\n",
    "        # Store the input layer activations\n",
    "        activations = inputs\n",
    "        self.activations[0][:-1] = activations\n",
    "        self.activations[0][-1] = 1\n",
    "        activations = np.append(activations, 1)\n",
    "\n",
    "        #print(self.activations)\n",
    "\n",
    "        for i, w in enumerate(self.weights):\n",
    "            # Calculate inputs h\n",
    "            #print(activations, i)\n",
    "            net_inputs = np.dot(activations, w)\n",
    "\n",
    "            # Calculate the activations\n",
    "            if i != len(self.weights) - 1:\n",
    "                activations = self._sigmoid(net_inputs)\n",
    "            else:\n",
    "                activations = net_inputs\n",
    "\n",
    "            # Save the activation for the current layer (if weights at W1, store a2)\n",
    "            self.activations[i+1][:-1] = activations\n",
    "            self.activations[i+1][-1] = 1\n",
    "            activations = np.append(activations, 1)\n",
    "\n",
    "        predictions = self._softmax(self.activations[-1][:-1])\n",
    "    \n",
    "        return predictions\n",
    "\n",
    "\n",
    "    def gradient_descent(self, learning_rate, debug = False):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * self.derivatives[i]\n",
    "\n",
    "\n",
    "    def train(self, inputs, targets, epochs = 10, learning_rate = 0.1, batch_size = 0):\n",
    "\n",
    "        for i in range(epochs):\n",
    "            sum_error = 0\n",
    "            for j in range(len(inputs)):\n",
    "\n",
    "                # Forward propagate\n",
    "                output = self.forward_propagate(inputs[j])\n",
    "\n",
    "                # Find derivative of loss function with respect to output layer\n",
    "                loss_derivative = output - targets[j]\n",
    "\n",
    "                # Back propagate to find the derivatives\n",
    "                self.back_propagate(loss_derivative)\n",
    "\n",
    "                # Gradient descent to update weights\n",
    "                self.gradient_descent(learning_rate)\n",
    "\n",
    "                # Calculate the error at this iteration\n",
    "                sum_error += self._mse(targets[j], output)\n",
    "            \n",
    "            # report error\n",
    "            print(f\"Epoch {i+1}, Mean Squared Error: {sum_error/len(inputs)}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _softmax(x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def _softmax_derivative(x):\n",
    "        s = x.reshape(-1,1)\n",
    "        return np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "    @staticmethod\n",
    "    def _mse(target, output):\n",
    "        return np.average((target - output) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid_derivative(x):\n",
    "        return x * (1.0 - x)\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(x):\n",
    "        return 1.0 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train_normalized = (x_train) / 255.0\n",
    "x_test_normalized = (x_test) / 255.0\n",
    "\n",
    "flattenedX = np.array([np.zeros(784) for _ in range(60000)])\n",
    "for i in range(len(x_train)):\n",
    "    flattenedX[i] = x_train_normalized[i].flatten()\n",
    "\n",
    "one_hot_y = np.array([np.zeros(10) for _ in range(60000)])\n",
    "for i in range(len(x_train)):\n",
    "    one_hot_y[i][y_train[i]] = 1\n",
    "\n",
    "flattenedTest = np.array([np.zeros(784) for _ in range(10000)])\n",
    "for i in range(len(flattenedTest)):\n",
    "    flattenedTest[i] = x_test_normalized[i].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mlp_data.pkl', 'rb') as input:\n",
    "    mlp = pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (784) into shape (783)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14416/3543886574.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_propagate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflattenedTest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprobability\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14416/2390266478.py\u001b[0m in \u001b[0;36mforward_propagate\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# Store the input layer activations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mactivations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mactivations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (784) into shape (783)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 2151\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(x_test_normalized[index])\n",
    "plt.title(y_test[index])\n",
    "\n",
    "output = mlp.forward_propagate(flattenedTest[index])\n",
    "prediction = np.argmax(output)\n",
    "probability = output[prediction]\n",
    "\n",
    "print(f\"Prediction: {prediction}, probability: {probability * 100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('lime')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
   }
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "33121ecfcfef617be9cee54901a88539f25a6f6617b81e9cc1fb23984e818646"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
