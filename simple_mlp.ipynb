{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd02db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random"
   ]
  },
  {
   "source": [
    "class MLP:\n",
    "\n",
    "    def __init__(self, num_inputs=3, hidden_layers=[2], num_outputs=2):\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        # create a generic representation of the layers\n",
    "        layers = [num_inputs] + hidden_layers + [num_outputs]\n",
    "\n",
    "        # create random connection weights for the layers\n",
    "        self.weights = [np.random.rand(layers[i] + 1, layers[i + 1]) for i in range(len(layers) - 1)]\n",
    "\n",
    "        # save derivatives per layer\n",
    "        self.derivatives = [np.zeros((layers[i] + 1, layers[i + 1])) for i in range(len(layers) - 1)]\n",
    "\n",
    "        # save activations per layer\n",
    "\n",
    "        #print(layers)\n",
    "        #self.activations = [np.zeros(layers[i] + 1) if i!= len(layers)-1 else np.zeros(layers[i]) for i in range(len(layers))]\n",
    "        self.activations = [np.zeros(layers[i] + 1) for i in range(len(layers))]\n",
    "\n",
    "    \n",
    "    def back_propagate(self, error, debug = False):\n",
    "\n",
    "        for i in reversed(range(len(self.derivatives))):\n",
    "            # If w2, take derivative of a3\n",
    "            # define delta as: dE/da_(i+1) * s'(a_i+1)\n",
    "            delta = error * self._sigmoid_derivative(self.activations[i+1])\n",
    "\n",
    "            # Converts delta from array [0.1, 0.2] to 1x2 matrix [[0.1, 0.2]]\n",
    "            delta_reshaped = delta.reshape(delta.shape[0],-1).T\n",
    "\n",
    "            # get activations for current layer\n",
    "            #current_activations = self.activations[i]\n",
    "\n",
    "            # Converts a_i from array [0.1, 0.2] to 2x1 matrix [[0.1], [0.2]]\n",
    "            current_activations = self.activations[i].reshape(self.activations[i].shape[0], -1)\n",
    "\n",
    "            # Get the matrix of derivatives for W_i\n",
    "            self.derivatives[i] = np.dot(current_activations, delta_reshaped)\n",
    "\n",
    "            # error for prev layer as: dE/da_(i+1) * s'(a_i+1) * W_i or delta * W_i\n",
    "            error = np.dot(delta, self.weights[i].T)\n",
    "\n",
    "            if debug:\n",
    "                print(f\"Derivatives for W{i}: {self.derivatives[i]}\")\n",
    "\n",
    "    def forward_propagate(self, inputs):\n",
    "\n",
    "        # Store the input layer activations\n",
    "        activations = inputs\n",
    "        self.activations[0] = activations\n",
    "\n",
    "        for i, w in enumerate(self.weights):\n",
    "            # Calculate inputs h\n",
    "            net_inputs = np.dot(activations, w)\n",
    "\n",
    "            # Calculate the activations\n",
    "            activations = self._sigmoid(net_inputs)\n",
    "\n",
    "            # Save the activation for the current layer (if weights at W1, store a2)\n",
    "            self.activations[i+1] = activations\n",
    "            \n",
    "        return activations\n",
    "\n",
    "\n",
    "    def gradient_descent(self, learning_rate, debug = False):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * self.derivatives[i]\n",
    "\n",
    "\n",
    "    def train(self, inputs, targets, epochs = 10, learning_rate = 0.1, batch_size = 0):\n",
    "\n",
    "        for i in range(epochs):\n",
    "            sum_error = 0\n",
    "            for j in range(len(inputs)):\n",
    "\n",
    "                # Forward propagate\n",
    "                output = self.forward_propagate(inputs[j])\n",
    "\n",
    "                # Find derivative of loss function with respect to output layer\n",
    "                loss_derivative = output - targets[j]\n",
    "\n",
    "                # Back propagate to find the derivatives\n",
    "                self.back_propagate(loss_derivative)\n",
    "\n",
    "                # Gradient descent to update weights\n",
    "                self.gradient_descent(learning_rate)\n",
    "\n",
    "                # Calculate the error at this iteration\n",
    "                sum_error += self._mse(targets[j], output)\n",
    "            \n",
    "            # report error\n",
    "            print(f\"Epoch {i}, Mean Squared Error: {sum_error/len(inputs)}\")\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _mse(target, output):\n",
    "        return np.average((target - output) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid_derivative(x):\n",
    "        return x * (1.0 - x)\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(x):\n",
    "        return 1.0 / (1 + np.exp(-x))\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.15617142, 0.12616124])"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "# Define data\n",
    "items = np.array([[random()/2 for _ in range(2)] for _ in range(1000)])\n",
    "targets = np.array([[i[0] * i[1]] for i in items])\n",
    "\n",
    "items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array([0., 0.]), array([0., 0.]), array([0.])]"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "mlp = MLP(num_inputs = 2, hidden_layers=[2], num_outputs = 1)\n",
    "mlp.activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0, Mean Squared Error: 0.05705631649368361\n",
      "Epoch 1, Mean Squared Error: 0.007070253526140477\n",
      "Epoch 2, Mean Squared Error: 0.004745998727957247\n",
      "Epoch 3, Mean Squared Error: 0.004110631768882561\n",
      "Epoch 4, Mean Squared Error: 0.0038501056274325473\n",
      "Epoch 5, Mean Squared Error: 0.0037179745766159544\n",
      "Epoch 6, Mean Squared Error: 0.0036401280860459333\n",
      "Epoch 7, Mean Squared Error: 0.0035882623997721443\n",
      "Epoch 8, Mean Squared Error: 0.003549853053061957\n",
      "Epoch 9, Mean Squared Error: 0.003518776657805114\n",
      "Epoch 10, Mean Squared Error: 0.0034918177773068195\n",
      "Epoch 11, Mean Squared Error: 0.003467196114032446\n",
      "Epoch 12, Mean Squared Error: 0.0034438855356632037\n",
      "Epoch 13, Mean Squared Error: 0.0034212760151423076\n",
      "Epoch 14, Mean Squared Error: 0.0033989961116732475\n",
      "Epoch 15, Mean Squared Error: 0.0033768154571243852\n",
      "Epoch 16, Mean Squared Error: 0.003354589198193887\n",
      "Epoch 17, Mean Squared Error: 0.003332225386643308\n",
      "Epoch 18, Mean Squared Error: 0.0033096653685644934\n",
      "Epoch 19, Mean Squared Error: 0.0032868717555472097\n",
      "Epoch 20, Mean Squared Error: 0.003263820927535861\n",
      "Epoch 21, Mean Squared Error: 0.003240498300054162\n",
      "Epoch 22, Mean Squared Error: 0.0032168953065068464\n",
      "Epoch 23, Mean Squared Error: 0.003193007459506475\n",
      "Epoch 24, Mean Squared Error: 0.003168833098820162\n",
      "Epoch 25, Mean Squared Error: 0.0031443725802211436\n",
      "Epoch 26, Mean Squared Error: 0.0031196277494502924\n",
      "Epoch 27, Mean Squared Error: 0.0030946016014691204\n",
      "Epoch 28, Mean Squared Error: 0.0030692980604886152\n",
      "Epoch 29, Mean Squared Error: 0.003043721838770984\n",
      "Epoch 30, Mean Squared Error: 0.0030178783466894067\n",
      "Epoch 31, Mean Squared Error: 0.002991773635926282\n",
      "Epoch 32, Mean Squared Error: 0.002965414363822305\n",
      "Epoch 33, Mean Squared Error: 0.002938807770911483\n",
      "Epoch 34, Mean Squared Error: 0.002911961666327826\n",
      "Epoch 35, Mean Squared Error: 0.002884884417523628\n",
      "Epoch 36, Mean Squared Error: 0.0028575849419043516\n",
      "Epoch 37, Mean Squared Error: 0.0028300726987623053\n",
      "Epoch 38, Mean Squared Error: 0.002802357680413703\n",
      "Epoch 39, Mean Squared Error: 0.002774450401796389\n",
      "Epoch 40, Mean Squared Error: 0.002746361888027963\n",
      "Epoch 41, Mean Squared Error: 0.002718103659592888\n",
      "Epoch 42, Mean Squared Error: 0.0026896877149478566\n",
      "Epoch 43, Mean Squared Error: 0.002661126510424521\n",
      "Epoch 44, Mean Squared Error: 0.0026324329373772406\n",
      "Epoch 45, Mean Squared Error: 0.0026036202965794546\n",
      "Epoch 46, Mean Squared Error: 0.0025747022699188337\n",
      "Epoch 47, Mean Squared Error: 0.0025456928894822843\n",
      "Epoch 48, Mean Squared Error: 0.002516606504158649\n",
      "Epoch 49, Mean Squared Error: 0.002487457743920195\n"
     ]
    }
   ],
   "source": [
    "mlp.train(items, targets, epochs=50, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.07460465])"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "mlp.forward_propagate(np.array([0.2853, 0.623]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}