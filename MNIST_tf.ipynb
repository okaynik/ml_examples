{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run on TensorFlow 2.x\n",
    "#%tensorflow_version 2.x\n",
    "#from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# The following lines adjust the granularity of reporting. \n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n",
    "\n",
    "# The following line improves formatting when ouputting NumPy arrays.\n",
    "np.set_printoptions(linewidth = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train_normalized = x_train / 255.0\n",
    "x_test_normalized = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(learning_rate):\n",
    "  model = tf.keras.models.Sequential()\n",
    "  model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
    "  model.add(tf.keras.layers.Dense(units=256, activation='relu'))\n",
    "  model.add(tf.keras.layers.Dropout(rate=0.4))\n",
    "  model.add(tf.keras.layers.Dense(units=10, activation='softmax'))     \n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
    "                loss=\"sparse_categorical_crossentropy\",\n",
    "                metrics=['accuracy'])\n",
    "  \n",
    "  return model    \n",
    "\n",
    "\n",
    "def train_model(model, train_features, train_label, epochs,\n",
    "                batch_size=None, validation_split=0.1):\n",
    "\n",
    "  history = model.fit(x=train_features, y=train_label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=True, \n",
    "                      validation_split=validation_split)\n",
    "  epochs = history.epoch\n",
    "  hist = pd.DataFrame(history.history)\n",
    "\n",
    "  return epochs, hist    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nm7uh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "12/12 [==============================] - 2s 31ms/step - loss: 1.0874 - accuracy: 0.6703 - val_loss: 0.3880 - val_accuracy: 0.8879\n",
      "Epoch 2/50\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.4337 - accuracy: 0.8695 - val_loss: 0.2852 - val_accuracy: 0.9196\n",
      "Epoch 3/50\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.3203 - accuracy: 0.9062 - val_loss: 0.2380 - val_accuracy: 0.9329\n",
      "Epoch 4/50\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.2669 - accuracy: 0.9229 - val_loss: 0.2017 - val_accuracy: 0.9427\n",
      "Epoch 5/50\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.2302 - accuracy: 0.9328 - val_loss: 0.1791 - val_accuracy: 0.9488\n",
      "Epoch 6/50\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.2038 - accuracy: 0.9420 - val_loss: 0.1631 - val_accuracy: 0.9535\n",
      "Epoch 7/50\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.1845 - accuracy: 0.9460 - val_loss: 0.1492 - val_accuracy: 0.9571\n",
      "Epoch 8/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.1712 - accuracy: 0.9510 - val_loss: 0.1390 - val_accuracy: 0.9603\n",
      "Epoch 9/50\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.1550 - accuracy: 0.9554 - val_loss: 0.1307 - val_accuracy: 0.9619\n",
      "Epoch 10/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.1438 - accuracy: 0.9585 - val_loss: 0.1239 - val_accuracy: 0.9624\n",
      "Epoch 11/50\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.1355 - accuracy: 0.9610 - val_loss: 0.1185 - val_accuracy: 0.9641\n",
      "Epoch 12/50\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.1282 - accuracy: 0.9629 - val_loss: 0.1129 - val_accuracy: 0.9660\n",
      "Epoch 13/50\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.1188 - accuracy: 0.9659 - val_loss: 0.1087 - val_accuracy: 0.9670\n",
      "Epoch 14/50\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.1128 - accuracy: 0.9667 - val_loss: 0.1037 - val_accuracy: 0.9682\n",
      "Epoch 15/50\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.1060 - accuracy: 0.9691 - val_loss: 0.1017 - val_accuracy: 0.9683\n",
      "Epoch 16/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.1008 - accuracy: 0.9703 - val_loss: 0.0996 - val_accuracy: 0.9697\n",
      "Epoch 17/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0953 - accuracy: 0.9716 - val_loss: 0.0958 - val_accuracy: 0.9714\n",
      "Epoch 18/50\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0924 - accuracy: 0.9732 - val_loss: 0.0930 - val_accuracy: 0.9718\n",
      "Epoch 19/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0879 - accuracy: 0.9741 - val_loss: 0.0910 - val_accuracy: 0.9725\n",
      "Epoch 20/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0834 - accuracy: 0.9755 - val_loss: 0.0889 - val_accuracy: 0.9728\n",
      "Epoch 21/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0787 - accuracy: 0.9769 - val_loss: 0.0869 - val_accuracy: 0.9736\n",
      "Epoch 22/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0756 - accuracy: 0.9777 - val_loss: 0.0862 - val_accuracy: 0.9739\n",
      "Epoch 23/50\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0726 - accuracy: 0.9790 - val_loss: 0.0852 - val_accuracy: 0.9751\n",
      "Epoch 24/50\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0715 - accuracy: 0.9781 - val_loss: 0.0832 - val_accuracy: 0.9752\n",
      "Epoch 25/50\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0672 - accuracy: 0.9798 - val_loss: 0.0817 - val_accuracy: 0.9759\n",
      "Epoch 26/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0645 - accuracy: 0.9805 - val_loss: 0.0811 - val_accuracy: 0.9761\n",
      "Epoch 27/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0642 - accuracy: 0.9814 - val_loss: 0.0796 - val_accuracy: 0.9764\n",
      "Epoch 28/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0619 - accuracy: 0.9819 - val_loss: 0.0798 - val_accuracy: 0.9768\n",
      "Epoch 29/50\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0600 - accuracy: 0.9825 - val_loss: 0.0784 - val_accuracy: 0.9774\n",
      "Epoch 30/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0572 - accuracy: 0.9829 - val_loss: 0.0779 - val_accuracy: 0.9772\n",
      "Epoch 31/50\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.0550 - accuracy: 0.9832 - val_loss: 0.0783 - val_accuracy: 0.9772\n",
      "Epoch 32/50\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.0528 - accuracy: 0.9839 - val_loss: 0.0770 - val_accuracy: 0.9778\n",
      "Epoch 33/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0519 - accuracy: 0.9844 - val_loss: 0.0758 - val_accuracy: 0.9772\n",
      "Epoch 34/50\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.0512 - accuracy: 0.9847 - val_loss: 0.0744 - val_accuracy: 0.9779\n",
      "Epoch 35/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0481 - accuracy: 0.9854 - val_loss: 0.0743 - val_accuracy: 0.9787\n",
      "Epoch 36/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0466 - accuracy: 0.9853 - val_loss: 0.0752 - val_accuracy: 0.9783\n",
      "Epoch 37/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0457 - accuracy: 0.9868 - val_loss: 0.0745 - val_accuracy: 0.9787\n",
      "Epoch 38/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0437 - accuracy: 0.9865 - val_loss: 0.0754 - val_accuracy: 0.9787\n",
      "Epoch 39/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0430 - accuracy: 0.9870 - val_loss: 0.0732 - val_accuracy: 0.9788\n",
      "Epoch 40/50\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.0420 - accuracy: 0.9874 - val_loss: 0.0728 - val_accuracy: 0.9794\n",
      "Epoch 41/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0401 - accuracy: 0.9877 - val_loss: 0.0735 - val_accuracy: 0.9793\n",
      "Epoch 42/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0378 - accuracy: 0.9891 - val_loss: 0.0749 - val_accuracy: 0.9783\n",
      "Epoch 43/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0378 - accuracy: 0.9892 - val_loss: 0.0732 - val_accuracy: 0.9793\n",
      "Epoch 44/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0375 - accuracy: 0.9889 - val_loss: 0.0722 - val_accuracy: 0.9799\n",
      "Epoch 45/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0361 - accuracy: 0.9898 - val_loss: 0.0717 - val_accuracy: 0.9793\n",
      "Epoch 46/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0342 - accuracy: 0.9904 - val_loss: 0.0721 - val_accuracy: 0.9794\n",
      "Epoch 47/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0347 - accuracy: 0.9894 - val_loss: 0.0733 - val_accuracy: 0.9797\n",
      "Epoch 48/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0326 - accuracy: 0.9908 - val_loss: 0.0726 - val_accuracy: 0.9791\n",
      "Epoch 49/50\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0325 - accuracy: 0.9901 - val_loss: 0.0734 - val_accuracy: 0.9792\n",
      "Epoch 50/50\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.0318 - accuracy: 0.9903 - val_loss: 0.0740 - val_accuracy: 0.9793\n",
      "\n",
      " Evaluate the new model against the test set:\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0680 - accuracy: 0.9796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06797206401824951, 0.9796000123023987]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.003\n",
    "epochs = 50\n",
    "batch_size = 4000\n",
    "validation_split = 0.2\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate)\n",
    "\n",
    "# Train the model on the normalized training set.\n",
    "epochs, hist = train_model(my_model, x_train_normalized, y_train, \n",
    "                           epochs, batch_size, validation_split)\n",
    "\n",
    "# Plot a graph of the metric vs. epochs.\n",
    "list_of_metrics_to_plot = ['accuracy']\n",
    "#plot_curve(epochs, hist, list_of_metrics_to_plot)\n",
    "\n",
    "# Evaluate against the test set.\n",
    "print(\"\\n Evaluate the new model against the test set:\")\n",
    "my_model.evaluate(x=x_test_normalized, y=y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 2, with the confidence of 99.99910593032837%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 512\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(x_test_normalized[index])\n",
    "plt.title(y_test[index])\n",
    "\n",
    "output = my_model.predict(np.array([x_test_normalized[index]]))\n",
    "prediction = np.argmax(output)\n",
    "probability = output[0][prediction]\n",
    "\n",
    "\n",
    "print(f\"Prediction: {prediction}, with the confidence of {probability * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect at index 115\n",
      "Incorrect at index 247\n",
      "Incorrect at index 259\n",
      "Incorrect at index 321\n",
      "Incorrect at index 340\n",
      "Incorrect at index 445\n",
      "Incorrect at index 448\n",
      "Incorrect at index 582\n",
      "Incorrect at index 619\n",
      "Incorrect at index 659\n",
      "Incorrect at index 684\n",
      "Incorrect at index 691\n",
      "Incorrect at index 720\n",
      "Incorrect at index 740\n",
      "Incorrect at index 956\n",
      "Incorrect at index 965\n",
      "Incorrect at index 1003\n",
      "Incorrect at index 1014\n",
      "Incorrect at index 1039\n",
      "Incorrect at index 1107\n",
      "Incorrect at index 1112\n",
      "Incorrect at index 1128\n",
      "Incorrect at index 1178\n",
      "Incorrect at index 1182\n",
      "Incorrect at index 1226\n",
      "Incorrect at index 1232\n",
      "Incorrect at index 1242\n",
      "Incorrect at index 1247\n",
      "Incorrect at index 1319\n",
      "Incorrect at index 1378\n",
      "Incorrect at index 1393\n",
      "Incorrect at index 1444\n",
      "Incorrect at index 1500\n",
      "Incorrect at index 1522\n",
      "Incorrect at index 1530\n",
      "Incorrect at index 1549\n",
      "Incorrect at index 1553\n",
      "Incorrect at index 1609\n",
      "Incorrect at index 1621\n",
      "Incorrect at index 1678\n",
      "Incorrect at index 1681\n",
      "Incorrect at index 1709\n",
      "Incorrect at index 1717\n",
      "Incorrect at index 1754\n",
      "Incorrect at index 1790\n",
      "Incorrect at index 1800\n",
      "Incorrect at index 1878\n",
      "Incorrect at index 1901\n",
      "Incorrect at index 1952\n",
      "Incorrect at index 2004\n",
      "Incorrect at index 2016\n",
      "Incorrect at index 2035\n",
      "Incorrect at index 2043\n",
      "Incorrect at index 2053\n",
      "Incorrect at index 2098\n",
      "Incorrect at index 2109\n",
      "Incorrect at index 2118\n",
      "Incorrect at index 2129\n",
      "Incorrect at index 2135\n",
      "Incorrect at index 2182\n",
      "Incorrect at index 2189\n",
      "Incorrect at index 2293\n",
      "Incorrect at index 2369\n",
      "Incorrect at index 2387\n",
      "Incorrect at index 2406\n",
      "Incorrect at index 2414\n",
      "Incorrect at index 2488\n",
      "Incorrect at index 2582\n",
      "Incorrect at index 2607\n",
      "Incorrect at index 2648\n",
      "Incorrect at index 2654\n",
      "Incorrect at index 2877\n",
      "Incorrect at index 2896\n",
      "Incorrect at index 2921\n",
      "Incorrect at index 2939\n",
      "Incorrect at index 2953\n",
      "Incorrect at index 2970\n",
      "Incorrect at index 3005\n",
      "Incorrect at index 3060\n",
      "Incorrect at index 3073\n",
      "Incorrect at index 3117\n",
      "Incorrect at index 3206\n",
      "Incorrect at index 3289\n",
      "Incorrect at index 3336\n",
      "Incorrect at index 3503\n",
      "Incorrect at index 3520\n",
      "Incorrect at index 3549\n",
      "Incorrect at index 3558\n",
      "Incorrect at index 3559\n",
      "Incorrect at index 3567\n",
      "Incorrect at index 3597\n",
      "Incorrect at index 3681\n",
      "Incorrect at index 3716\n",
      "Incorrect at index 3727\n",
      "Incorrect at index 3751\n",
      "Incorrect at index 3757\n",
      "Incorrect at index 3780\n",
      "Incorrect at index 3808\n",
      "Incorrect at index 3811\n",
      "Incorrect at index 3818\n",
      "Incorrect at index 3838\n",
      "Incorrect at index 3853\n",
      "Incorrect at index 3869\n",
      "Incorrect at index 3893\n",
      "Incorrect at index 3902\n",
      "Incorrect at index 3906\n",
      "Incorrect at index 3926\n",
      "Incorrect at index 3941\n",
      "Incorrect at index 3943\n",
      "Incorrect at index 3946\n",
      "Incorrect at index 3985\n",
      "Incorrect at index 4063\n",
      "Incorrect at index 4065\n",
      "Incorrect at index 4075\n",
      "Incorrect at index 4078\n",
      "Incorrect at index 4163\n",
      "Incorrect at index 4176\n",
      "Incorrect at index 4199\n",
      "Incorrect at index 4201\n",
      "Incorrect at index 4224\n",
      "Incorrect at index 4248\n",
      "Incorrect at index 4271\n",
      "Incorrect at index 4289\n",
      "Incorrect at index 4355\n",
      "Incorrect at index 4360\n",
      "Incorrect at index 4369\n",
      "Incorrect at index 4425\n",
      "Incorrect at index 4433\n",
      "Incorrect at index 4443\n",
      "Incorrect at index 4497\n",
      "Incorrect at index 4500\n",
      "Incorrect at index 4536\n",
      "Incorrect at index 4731\n",
      "Incorrect at index 4740\n",
      "Incorrect at index 4761\n",
      "Incorrect at index 4807\n",
      "Incorrect at index 4814\n",
      "Incorrect at index 4823\n",
      "Incorrect at index 4880\n",
      "Incorrect at index 4956\n",
      "Incorrect at index 4966\n",
      "Incorrect at index 5078\n",
      "Incorrect at index 5140\n",
      "Incorrect at index 5331\n",
      "Incorrect at index 5457\n",
      "Incorrect at index 5634\n",
      "Incorrect at index 5642\n",
      "Incorrect at index 5676\n",
      "Incorrect at index 5734\n",
      "Incorrect at index 5749\n",
      "Incorrect at index 5757\n",
      "Incorrect at index 5842\n",
      "Incorrect at index 5887\n",
      "Incorrect at index 5888\n",
      "Incorrect at index 5936\n",
      "Incorrect at index 5937\n",
      "Incorrect at index 5955\n",
      "Incorrect at index 5972\n",
      "Incorrect at index 5973\n",
      "Incorrect at index 6059\n",
      "Incorrect at index 6071\n",
      "Incorrect at index 6166\n",
      "Incorrect at index 6168\n",
      "Incorrect at index 6173\n",
      "Incorrect at index 6400\n",
      "Incorrect at index 6555\n",
      "Incorrect at index 6571\n",
      "Incorrect at index 6574\n",
      "Incorrect at index 6597\n",
      "Incorrect at index 6608\n",
      "Incorrect at index 6625\n",
      "Incorrect at index 6651\n",
      "Incorrect at index 6755\n",
      "Incorrect at index 6783\n",
      "Incorrect at index 6847\n",
      "Incorrect at index 7216\n",
      "Incorrect at index 7921\n",
      "Incorrect at index 8062\n",
      "Incorrect at index 8094\n",
      "Incorrect at index 8325\n",
      "Incorrect at index 8522\n",
      "Incorrect at index 8527\n",
      "Incorrect at index 8863\n",
      "Incorrect at index 9009\n",
      "Incorrect at index 9015\n",
      "Incorrect at index 9019\n",
      "Incorrect at index 9024\n",
      "Incorrect at index 9280\n",
      "Incorrect at index 9587\n",
      "Incorrect at index 9634\n",
      "Incorrect at index 9642\n",
      "Incorrect at index 9664\n",
      "Incorrect at index 9679\n",
      "Incorrect at index 9692\n",
      "Incorrect at index 9698\n",
      "Incorrect at index 9700\n",
      "Incorrect at index 9729\n",
      "Incorrect at index 9745\n",
      "Incorrect at index 9768\n",
      "Incorrect at index 9770\n",
      "Incorrect at index 9792\n",
      "Incorrect at index 9808\n",
      "Incorrect at index 9839\n",
      "Incorrect at index 9982\n"
     ]
    }
   ],
   "source": [
    "predictions = my_model.predict(x_test_normalized)\n",
    "for i in range(len(x_test_normalized)):\n",
    "    if np.argmax(predictions[i]) != y_test[i]:\n",
    "        print(f\"Incorrect at index {i}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.0983873e-08, 2.5634981e-09, 1.0956786e-06, ..., 9.9948478e-01, 1.6271198e-07, 2.1385183e-06],\n",
       "       [5.1790672e-10, 1.3950883e-04, 9.9986029e-01, ..., 6.3155678e-14, 3.6846486e-09, 2.0551755e-15],\n",
       "       [1.6311424e-07, 9.9945337e-01, 5.6200108e-05, ..., 4.0978985e-04, 6.0010072e-05, 2.4575786e-07],\n",
       "       ...,\n",
       "       [4.9508054e-12, 2.9768316e-10, 4.9946266e-11, ..., 5.3813750e-07, 4.1265787e-07, 2.4981762e-06],\n",
       "       [1.1314777e-09, 4.5201607e-09, 1.0157545e-12, ..., 1.1242662e-10, 1.1867797e-05, 6.4432583e-12],\n",
       "       [1.8315756e-08, 8.2585105e-11, 1.0274272e-07, ..., 2.1637594e-14, 2.0218038e-09, 1.9474060e-11]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('lime')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
   }
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "33121ecfcfef617be9cee54901a88539f25a6f6617b81e9cc1fb23984e818646"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
